{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T17:55:43.999712Z",
     "iopub.status.busy": "2026-01-16T17:55:43.998517Z",
     "iopub.status.idle": "2026-01-16T17:55:44.004936Z",
     "shell.execute_reply": "2026-01-16T17:55:44.003829Z",
     "shell.execute_reply.started": "2026-01-16T17:55:43.999666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T17:55:45.643158Z",
     "iopub.status.busy": "2026-01-16T17:55:45.642792Z",
     "iopub.status.idle": "2026-01-16T17:55:49.763351Z",
     "shell.execute_reply": "2026-01-16T17:55:49.762059Z",
     "shell.execute_reply.started": "2026-01-16T17:55:45.643095Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T18:01:00.899900Z",
     "iopub.status.busy": "2026-01-16T18:01:00.898379Z",
     "iopub.status.idle": "2026-01-16T18:01:01.215703Z",
     "shell.execute_reply": "2026-01-16T18:01:01.214609Z",
     "shell.execute_reply.started": "2026-01-16T18:01:00.899837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from typing import Optional, Dict, List\n",
    "from tqdm.auto import tqdm\n",
    "from openai import OpenAI, APIError, RateLimitError, APITimeoutError\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "class Config:\n",
    "    INPUT_PARQUET = \"/kaggle/input/avito-data/data/test_part_0001.snappy.parquet\"\n",
    "    OUTPUT_DATASET = \"avito_train_dataset.jsonl\"\n",
    "    LOG_FILE = \"generation_process.log\"\n",
    "    \n",
    "    SAMPLE_SIZE = 1500\n",
    "    MIN_LEN = 30\n",
    "    MAX_LEN = 150\n",
    "    \n",
    "    MODEL_NAME = \"deepseek/deepseek-v3.2-speciale\"\n",
    "    API_BASE = \"https://openrouter.ai/api/v1\"\n",
    "    \n",
    "    TEMPERATURE = 0.9\n",
    "    TOP_P = 0.9\n",
    "    FREQ_PENALTY = 0.1\n",
    "    PRESENCE_PENALTY = 0.1\n",
    "    REP_PENALTY = 1.05\n",
    "    REASONING_EFFORT = \"low\"\n",
    "    \n",
    "    MAX_RETRIES = 5\n",
    "    RETRY_DELAY = 2\n",
    "\n",
    "# --- LOGGING SETUP ---\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "file_handler = logging.FileHandler(Config.LOG_FILE, mode='a', encoding='utf-8')\n",
    "file_handler.setFormatter(log_formatter)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(log_formatter)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "logger.info(\"Инициализация скрипта генерации запущена.\")\n",
    "\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    api_key = user_secrets.get_secret(\"OPENROUTER_API_KEY\")\n",
    "    client = OpenAI(base_url=Config.API_BASE, api_key=api_key)\n",
    "    logger.info(\"OpenAI client успешно настроен.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Не удалось получить API Key из Secrets: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T18:01:54.274268Z",
     "iopub.status.busy": "2026-01-16T18:01:54.273917Z",
     "iopub.status.idle": "2026-01-16T18:01:59.437342Z",
     "shell.execute_reply": "2026-01-16T18:01:59.436338Z",
     "shell.execute_reply.started": "2026-01-16T18:01:54.274242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_filter_data():\n",
    "    logger.info(\"Загрузка данных из Parquet...\")\n",
    "    \n",
    "    PREVIOUS_PROGRESS_PATH = \"/kaggle/input/avito-700-samples/avito_train_dataset-united.jsonl\"\n",
    "    \n",
    "    cols = [\"base_item_id\", \"base_title\", \"base_description\", \n",
    "            \"base_category_name\", \"base_subcategory_name\"]\n",
    "    \n",
    "    df = pd.read_parquet(Config.INPUT_PARQUET, columns=cols)\n",
    "    \n",
    "    df['desc_len'] = df['base_description'].str.len().fillna(0)\n",
    "    filtered_df = df[\n",
    "        (df['desc_len'] >= Config.MIN_LEN) & \n",
    "        (df['desc_len'] <= Config.MAX_LEN)\n",
    "    ].copy()\n",
    "    \n",
    "    processed_ids = set()\n",
    "    \n",
    "    if os.path.exists(PREVIOUS_PROGRESS_PATH):\n",
    "        logger.info(f\"Считываем ID из загруженного датасета: {PREVIOUS_PROGRESS_PATH}\")\n",
    "        with open(PREVIOUS_PROGRESS_PATH, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    processed_ids.add(record.get('id'))\n",
    "                except: continue\n",
    "                \n",
    "    if os.path.exists(Config.OUTPUT_DATASET):\n",
    "        logger.info(f\"Считываем ID из локального файла: {Config.OUTPUT_DATASET}\")\n",
    "        with open(Config.OUTPUT_DATASET, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    processed_ids.add(record.get('id'))\n",
    "                except: continue\n",
    "    \n",
    "    logger.info(f\"Всего уже обработано ранее: {len(processed_ids)} записей.\")\n",
    "    \n",
    "    filtered_df = filtered_df[~filtered_df['base_item_id'].isin(processed_ids)]\n",
    "    \n",
    "    remaining_needed = Config.SAMPLE_SIZE - len(processed_ids)\n",
    "    if remaining_needed <= 0:\n",
    "        logger.info(\"Цель по количеству записей уже достигнута!\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    if len(filtered_df) > remaining_needed:\n",
    "        filtered_df = filtered_df.sample(remaining_needed, random_state=42)\n",
    "        \n",
    "    logger.info(f\"Осталось доделать: {len(filtered_df)} записей.\")\n",
    "    return filtered_df\n",
    "\n",
    "work_df = load_and_filter_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T18:02:30.423050Z",
     "iopub.status.busy": "2026-01-16T18:02:30.421962Z",
     "iopub.status.idle": "2026-01-16T18:02:30.434660Z",
     "shell.execute_reply": "2026-01-16T18:02:30.433546Z",
     "shell.execute_reply.started": "2026-01-16T18:02:30.423009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DescriptionGenerator:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.system_prompt = (\n",
    "            \"Ты — ведущий AI-копирайтер платформы Авито. Твоя задача: профессионально отредактировать описание товара, \"\n",
    "            \"сделав его привлекательным, структурированным и грамотным.\\n\\n\"\n",
    "            \"СЛЕДУЙ ПРАВИЛАМ СТРОГО:\\n\"\n",
    "            \"1. ИСПРАВЛЕНИЕ: Исправь все опечатки, пунктуационные и грамматические ошибки исходного текста.\\n\"\n",
    "            \"2. ФАКТЫ: Оставь все детали из исходного описания. Категорически запрещено выдумывать состояние товара \"\n",
    "            \"или детали, которых нет в тексте.\\n\"\n",
    "            \"3. ЭКСПЕРТНОСТЬ: Добавь 1-2 предложения с общеизвестными преимуществами данной модели товара.\\n\"\n",
    "            \"4. СТРУКТУРА:\\n\"\n",
    "            \"   - Разбей текст на логические абзацы.\\n\"\n",
    "            \"   - Обязательно добавь один маркированный список (через точку '•').\\n\"\n",
    "            \"   - Лимит эмодзи: строго не более 2 штук.\\n\"\n",
    "            \"5. СТИЛЬ: Профессиональный, человечный, лаконичный.\\n\"\n",
    "            \"6. ОГРАНИЧЕНИЕ ДЛИНЫ: Ориентируйся на объем 40–80 слов. \"\n",
    "            \"ВАЖНО: Если исходное описание очень короткое, не пытайся растягивать текст «водой» или выдумками. \"\n",
    "            \"В таком случае лучше оставить описание коротким (20-40 слов), но честным и полезным.\\n\\n\"\n",
    "            \"Выдавай ТОЛЬКО финальный текст объявления без вводных фраз.\"\n",
    "        )\n",
    "\n",
    "    def _create_user_prompt(self, row):\n",
    "        return (\n",
    "            f\"Улучши следующее объявление:\\n\"\n",
    "            f\"Категория: {row['base_category_name']} / {row['base_subcategory_name']}\\n\"\n",
    "            f\"Заголовок: {row['base_title']}\\n\"\n",
    "            f\"Исходное описание: {row['base_description']}\\n\\n\"\n",
    "            \"Результат:\"\n",
    "        )\n",
    "\n",
    "    def generate(self, row) -> Optional[str]:\n",
    "        prompt = self._create_user_prompt(row)\n",
    "        \n",
    "        for attempt in range(Config.MAX_RETRIES):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=Config.MODEL_NAME,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=Config.TEMPERATURE,\n",
    "                    top_p=Config.TOP_P,\n",
    "                    frequency_penalty=Config.FREQ_PENALTY,\n",
    "                    presence_penalty=Config.PRESENCE_PENALTY,\n",
    "                    extra_body={\"reasoning_effort\": Config.REASONING_EFFORT},\n",
    "                    extra_headers={\n",
    "                        \"HTTP-Referer\": \"https://kaggle.com\",\n",
    "                        \"X-Title\": \"Avito Portfolio Project\"\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Проверка на пустой ответ\n",
    "                content = response.choices[0].message.content\n",
    "                \n",
    "                if content:\n",
    "                    return content.strip()\n",
    "                else:\n",
    "                    return None\n",
    "            \n",
    "            except APITimeoutError:\n",
    "                logger.warning(f\"Timeout ID {row['base_item_id']}. Retry {attempt+1}...\")\n",
    "                time.sleep(2)\n",
    "            except RateLimitError:\n",
    "                logger.warning(f\"RateLimit ID {row['base_item_id']}. Sleep 5s...\")\n",
    "                time.sleep(5)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error ID {row['base_item_id']}: {e}\")\n",
    "                return None\n",
    "                \n",
    "        logger.error(f\"Не удалось получить ответ для ID {row['base_item_id']} после всех попыток.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not work_df.empty:\n",
    "    generator = DescriptionGenerator(client)\n",
    "    \n",
    "    with open(Config.OUTPUT_DATASET, 'a', encoding='utf-8', buffering=1) as f_out:\n",
    "        \n",
    "        logger.info(\"Начинаем генерацию...\")\n",
    "        \n",
    "        for _, row in tqdm(work_df.iterrows(), total=len(work_df), desc=\"Generating\"):\n",
    "            \n",
    "            generated_text = generator.generate(row)\n",
    "            \n",
    "            if generated_text:\n",
    "                record = {\n",
    "                    \"id\": row['base_item_id'],\n",
    "                    \"category_context\": f\"{row['base_category_name']} / {row['base_subcategory_name']}\",\n",
    "                    \"title\": row['base_title'],\n",
    "                    \"original_description\": row['base_description'],\n",
    "                    \"generated_description\": generated_text,\n",
    "                    \"instruction\": \"Улучши описание объявления для Авито, исправь ошибки и добавь структуру.\"\n",
    "                }\n",
    "                \n",
    "                f_out.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                logger.info(f\"\\n[ID: {row['base_item_id']}] SUCCESS\")\n",
    "                logger.info(f\"ORIGINAL: {row['base_description'][:100]}...\")\n",
    "                logger.info(f\"GENERATED: {generated_text[:100]}...\")\n",
    "                \n",
    "            else:\n",
    "                logger.error(f\"[ID: {row['base_item_id']}] FAILED to generate.\")\n",
    "\n",
    "    logger.info(\"Обработка завершена.\")\n",
    "else:\n",
    "    logger.info(\"Нет данных для обработки.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(Config.OUTPUT_DATASET):\n",
    "    print(\"--- ПРИМЕРЫ ИЗ СОЗДАННОГО ДАТАСЕТА ---\")\n",
    "    with open(Config.OUTPUT_DATASET, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 3: break\n",
    "            data = json.loads(line)\n",
    "            print(f\"\\nID: {data['id']}\")\n",
    "            print(f\"INPUT: {data['original_description']}\")\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"OUTPUT: {data['generated_description']}\")\n",
    "            print(\"=\" * 40)\n",
    "else:\n",
    "    print(\"Файл датасета не найден.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "final_file = \"final_avito_dataset_1500.jsonl\"\n",
    "old_file = \"/kaggle/input/avito-processed-data/avito_train_dataset.jsonl\"\n",
    "new_file = \"avito_train_dataset.jsonl\"\n",
    "\n",
    "with open(final_file, 'w', encoding='utf-8') as f_out:\n",
    "    if os.path.exists(old_file):\n",
    "        with open(old_file, 'r') as f_in:\n",
    "            f_out.write(f_in.read())\n",
    "    if os.path.exists(new_file):\n",
    "        with open(new_file, 'r') as f_in:\n",
    "            f_out.write(f_in.read())\n",
    "\n",
    "print(f\"Готово! Финальный файл: {final_file}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7059942,
     "sourceId": 11290637,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9276840,
     "sourceId": 14524940,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9280944,
     "sourceId": 14530975,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9283518,
     "sourceId": 14535248,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
